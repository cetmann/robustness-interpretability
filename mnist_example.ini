[PATHS]
# The used dataset. Currently only supports 'TinyImageNet', 'MNIST'
# and 'ImageNet'.
dataset_name = MNIST
# The folder in which to save the different tensorboard summaries.
# For a cohort of experiments, this should ideally be the same.
tensorboard_logdir = /localdata/logs/
# The folder in which to save the different tensorboard summaries.
# For a cohort of experiments, this should ideally be the same.
saved_model_folder = /localdata/models/

[ARCHITECTURE]
# Possible values: VGG16, ResNet18 or ResNet50
model = SmallNet
# Do we want to use a pretrained model? ONLY works
# if we use ResNet50 AND ImageNet data.
pretrained = False

[HYPERPARAMETERS]
# The total number of epochs.
num_epochs = 200
# The learning rate at the beginning of training.
learning_rate_at_start = 0.001
# Every lr_decrease_interval epochs, multiply the
# current learning rate by lr_decrease_factor.
# If you don't want this, simply set lr_decrease_interval
# higher than num_epochs.
lr_decrease_interval = 80
lr_decrease_factor = .1
# Batch size per GPU.
batch_size_per_gpu = 100
# Whether to penalize a weighted sum of 1-norms of
# wavelet coefficients of the saliency. 'False' is faster
# than just setting the multipliers to zero.
robust_regularization = True
# Whether to decompose \nabla_x L into its wavelet coefficients.
# If 'False', the regularization is applied to just the image.
use_wavelet_decomposition = False
# Define with respect to which output the gradients are
# calculated. 'logits' or 'NLL'.
sensitivity_mode = NLL
# Multiplier in front of the penalty term. Model starts to degenerate at 1000. Name is a misnomer currently.
lp_wavelet_parameter = 10.
# Which p-norm to use
p_norm = 2
# Squared 2-norm weight penalty parameter.
weight_decay_parameter = 0.000001
# Exponential moving average multiplier for the batch
# normalization layers, if present.
bn_momentum_value = .99
# Whether to set K.learning_phase() to 0 or 1 when
# training. If we train batch normalized networks,
# setting this to 0 has the effect of using the
# running mean statistics instead of the batch
# statistics, which is more stable but slower in the
# beginning.
learning_phase = 1

[LOGGING]
# Execute the training summary operator every
# train_summary_period batches.
train_summary_period = 50
# Execute the training summary operator every
# val_summary_period batches.
val_summary_period = 300
# When checking the adversarial vulnerability,
# try the different attacks on num_adversarial_batches
# batches of the img_data set. This may take a considerable 
# amount of time so don't set this too high.
num_adversarial_batches = 1
# Execute the training summary operator every
# train_summary_period batches.
adversarial_test_period = 5000000
